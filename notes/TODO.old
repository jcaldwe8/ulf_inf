- Write bash python script for all the sampling so that I can easily run it after making appropraite changes.
  - include prior normalization step as an option


To talk about during meeting:
  - Copy down the problematic issues I found from Stratos inference rules.
  - Some of the inferences require something like "immediately following" or
    "during the same interval as the event in utterance".  Do you have any good
    ideas of how to handle this?  I suppose we could add that into the ULF
    inference, but that seems like it would get overly complicated and messy
    really quick.
  - Some of the tricky cases while mapping inference rules
  - If I get a chance to think about it, how to handle possible presupposition
    diffusion.  Relatedly, that factives such as 'know' tend to express the
    speaker's beliefs, not necessarily world-truth.  Can also be attribution:
    "The police knows the bomb was homemade" -- the speaker doesn't have the
    evidence, but believes the police does.  Similar to "The police report says
    the bomb was homemade."  Also, "she knows that her actions are irresponsible"
    is reporting another person's state.

Over spring break:
  - Read through naming & necessity.  It's so long I'll need to digest it a bit more 
    before I have a coherent opinion about it. 
  - Started cleaning up implicative rules from Stratos to fit ULF inference
    (tense and progressive).  Perfective seemed problematic since it requires
    explicitly relating to the original utterance time, which ULF doesn't have
    a good way to do.
    

By Friday:
  - Set up website for inference annotation (with the implicative annotation
    instructions integrated).
By next week:
  - Do some preliminary inference annotation and debug issues
  - Setup ULF annotation website (still haven't done it from last semester...)


Now
* fix 'confess'
* Finish mapping implication rules
* Add complete berate non-complementizer constructions
* Flesh out passive construction mappings
* Look at other TODOs
* Setup website for doing inference annotations
* Figure out exact inference rule format.  May need to relax some of the ULF
  constraints for simplify the tense/aspect positions so we don't need to make
  overly complex rules.  For example, if we have a variable of a VP type, have
  a simple way to make it tensed (rather than having to take it apart and find
  the head verb lexeme.

* Fix project gutenberg sentence bound for mrs.
* Make a more complete version of the human inference guidelines
* Add in Tatoeba to all these datasets
* Write instructions for cleaning up dialogue data (precursor to ULF annotation)


* Sample from project gutenberg, discourse treebank, and switchboard and UIUC QA (write code for it)
* Make sure patterns are reasonable for each set
  Take a sample of 50 for each phenomenon in each dataset and see what proportion actually falls into the category we're interested in.
* Write up initial distinction between inference types (entailment, presupposition, implicature, etc.) since they seem to have different conditions on failure
* Once the dataset balance has been decided on, build dataset and split each sample into development and actual test data (dev should be maybe 10%?) this way we can build the inference engine without knowing the test data.
* Generalizing sampling from switchboard so that the sample matching simplifies 
  the sentnece (remove repeated words and disfluencies) but keeps the original
  sentence in the stored values so that those can be correctly fixed manually. -- just allow the system to match as necessary while avoiding using it for actual correction.
* Get sample of patterns from each
Tomorrow
* Address Len's critique of the guidelines and make a complete version

3/6/2018 Completed
* removed "have" since it seems to allow similar inferences to implicatives through the auxiliary, but not base verb form.
* added clause that ignores "you know" from matching "know" if a flag is set.
3/7/2018 Completed
* Expanded contractions in switchboard (and punctuation expansion, but I don't think switchboard has that)
* Added casing-based swtichboard sentence delimiting -- they are a bit long, but more coherent.
* Added flexible ? matching for questions since it's a low false-positive feature
* Checked question sample over UIUC -- we get full coverage except for imperatives "Name a famous martyr", "Describe cosmology"
  A bit worried because this dataset contains knowledge base questions: "What
  is the state nickname of Mississippi?" which don't have much interesting
  going on in the dialogue such as "What are they doing?" or "Do you take me
  for a dunce?" which have some conversational complexities.
* Matching processing in switchboard first removes disfluencies and repeated
  phrases so that they don't get in the way of the pattern matches.
  -- we can use this script to give a preliminary disfluency reduction that can be corrected.
  This removes erroneous request patterns where repeated phrases look like inverted structure: "I can I can go home" -> "... can I ..."
  This also captures many more questions that were missed because of utterances that start with a disfluency
* Corrected sentence boundary detection in Project Gutenberg




Short-term
* Look at elemma and uppen impl lists and make sure there aren't any major inconsistencies.
* Merge uppen_morph handling with other resources into a more integrated codebase.
* Getting LOTS of false positives on switchboard requests
* Add multiword coverage to Uppen Morph data.
* Figure out the relationship between impl-list.txt, imp.lisp, and weak-imp.lisp in stratos-data.
* Look into Jon's Children's book list for Project Gutenberg


Long-term
* Maybe think of a better structure to the datasets so that the ignored files
can all sit in the same location (so we don't need to update the gitignore
every time we add a dataset).

* Fix sentence boundary identification for Project Gutenberg by adding more
* robust abbreviation identification (XXX.).

* Maybe look into modified factives.  Even under intensional modification
  factives (or at least some of them) seem to preserve some presuppositions,
  particulary factive presuppositions.  For example,
    "The police officer almost ascertained that I was drunk" -> "I was drunk"
    "I almost knew that you were the mystery person." -> "You were the mystery person."
    "She was probably amused that I got pranked." -> "I got pranked."



